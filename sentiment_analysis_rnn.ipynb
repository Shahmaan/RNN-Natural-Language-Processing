{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFq2fry8we47Wx3sDxD4yx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Sentiment Analysis RNN\n","Recurrent neural network in action. For this example, we are going to do something called sentiment analysis.\n","\n","The formal definition of this term from Wikipedia is as follows:\n","\n","*the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.*\n","\n","The example weâ€™ll use here is classifying movie reviews as either postive, negative or neutral.\n","\n","*This guide is based on the following tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn*\n","\n"],"metadata":{"id":"DV0urc7-KI9y"}},{"cell_type":"markdown","source":["### Movie Review Dataset\n","Well start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."],"metadata":{"id":"APTsPXMhKMDa"}},{"cell_type":"code","source":["from keras.datasets import imdb\n","import keras\n","import tensorflow as tf\n","import os\n","import numpy as np\n","\n","VOCAB_SIZE = 88584\n","\n","MAXLEN = 250\n","BATCH_SIZE = 64"],"metadata":{"id":"b3PcCnrWKMOH","executionInfo":{"status":"ok","timestamp":1677707522374,"user_tz":-60,"elapsed":8119,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# importing the data\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9v5tK-FeKNeI","executionInfo":{"status":"ok","timestamp":1677707540101,"user_tz":-60,"elapsed":7924,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"ff3e5e69-1254-426a-8d20-bbc4a23679d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["# Let's look at what a review looks like\n","train_data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJEbABd1KVST","executionInfo":{"status":"ok","timestamp":1677707542781,"user_tz":-60,"elapsed":377,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"2f6d0d3a-3c3d-4bec-9ddb-272b9700c2e6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 14,\n"," 22,\n"," 16,\n"," 43,\n"," 530,\n"," 973,\n"," 1622,\n"," 1385,\n"," 65,\n"," 458,\n"," 4468,\n"," 66,\n"," 3941,\n"," 4,\n"," 173,\n"," 36,\n"," 256,\n"," 5,\n"," 25,\n"," 100,\n"," 43,\n"," 838,\n"," 112,\n"," 50,\n"," 670,\n"," 22665,\n"," 9,\n"," 35,\n"," 480,\n"," 284,\n"," 5,\n"," 150,\n"," 4,\n"," 172,\n"," 112,\n"," 167,\n"," 21631,\n"," 336,\n"," 385,\n"," 39,\n"," 4,\n"," 172,\n"," 4536,\n"," 1111,\n"," 17,\n"," 546,\n"," 38,\n"," 13,\n"," 447,\n"," 4,\n"," 192,\n"," 50,\n"," 16,\n"," 6,\n"," 147,\n"," 2025,\n"," 19,\n"," 14,\n"," 22,\n"," 4,\n"," 1920,\n"," 4613,\n"," 469,\n"," 4,\n"," 22,\n"," 71,\n"," 87,\n"," 12,\n"," 16,\n"," 43,\n"," 530,\n"," 38,\n"," 76,\n"," 15,\n"," 13,\n"," 1247,\n"," 4,\n"," 22,\n"," 17,\n"," 515,\n"," 17,\n"," 12,\n"," 16,\n"," 626,\n"," 18,\n"," 19193,\n"," 5,\n"," 62,\n"," 386,\n"," 12,\n"," 8,\n"," 316,\n"," 8,\n"," 106,\n"," 5,\n"," 4,\n"," 2223,\n"," 5244,\n"," 16,\n"," 480,\n"," 66,\n"," 3785,\n"," 33,\n"," 4,\n"," 130,\n"," 12,\n"," 16,\n"," 38,\n"," 619,\n"," 5,\n"," 25,\n"," 124,\n"," 51,\n"," 36,\n"," 135,\n"," 48,\n"," 25,\n"," 1415,\n"," 33,\n"," 6,\n"," 22,\n"," 12,\n"," 215,\n"," 28,\n"," 77,\n"," 52,\n"," 5,\n"," 14,\n"," 407,\n"," 16,\n"," 82,\n"," 10311,\n"," 8,\n"," 4,\n"," 107,\n"," 117,\n"," 5952,\n"," 15,\n"," 256,\n"," 4,\n"," 31050,\n"," 7,\n"," 3766,\n"," 5,\n"," 723,\n"," 36,\n"," 71,\n"," 43,\n"," 530,\n"," 476,\n"," 26,\n"," 400,\n"," 317,\n"," 46,\n"," 7,\n"," 4,\n"," 12118,\n"," 1029,\n"," 13,\n"," 104,\n"," 88,\n"," 4,\n"," 381,\n"," 15,\n"," 297,\n"," 98,\n"," 32,\n"," 2071,\n"," 56,\n"," 26,\n"," 141,\n"," 6,\n"," 194,\n"," 7486,\n"," 18,\n"," 4,\n"," 226,\n"," 22,\n"," 21,\n"," 134,\n"," 476,\n"," 26,\n"," 480,\n"," 5,\n"," 144,\n"," 30,\n"," 5535,\n"," 18,\n"," 51,\n"," 36,\n"," 28,\n"," 224,\n"," 92,\n"," 25,\n"," 104,\n"," 4,\n"," 226,\n"," 65,\n"," 16,\n"," 38,\n"," 1334,\n"," 88,\n"," 12,\n"," 16,\n"," 283,\n"," 5,\n"," 16,\n"," 4472,\n"," 113,\n"," 103,\n"," 32,\n"," 15,\n"," 16,\n"," 5345,\n"," 19,\n"," 178,\n"," 32]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# All the reviews have different word lengths which we have to account for\n","\n","len(train_data[0]), len(train_data[1]), len(train_data[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92E3Q-NaKX09","executionInfo":{"status":"ok","timestamp":1677707553574,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"30cc603b-07ac-46a3-bd4a-a1ef86c1295e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(218, 189, 141)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### More Preprocessing\n","If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n","- if the review is greater than 250 words then trim off the extra words\n","- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250.\n","\n","Luckily for us keras has a function that can do this for us:"],"metadata":{"id":"YVhQz55MKcFA"}},{"cell_type":"code","source":["train_data = keras.utils.pad_sequences(train_data, MAXLEN)\n","test_data = keras.utils.pad_sequences(test_data, MAXLEN)"],"metadata":{"id":"AhUwD9-vKagN","executionInfo":{"status":"ok","timestamp":1677707566100,"user_tz":-60,"elapsed":862,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Let's look at the length now!\n","\n","len(train_data[0]), len(train_data[1]), len(train_data[2]) # As we can see they are all equal to the length of 250."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACXT2Sf5KdWQ","executionInfo":{"status":"ok","timestamp":1677707583785,"user_tz":-60,"elapsed":4,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"430a2232-51f5-44ea-aa6d-9b57df7d3451"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(250, 250, 250)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# If we look closer on the first review we can see that there are a bunch of 0's. A padding was added because the review had less than 250 words.\n","train_data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lR-lalnKh1U","executionInfo":{"status":"ok","timestamp":1677707591124,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"063e6107-b028-47b3-f127-9bc8934107a0"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     1,    14,    22,    16,\n","          43,   530,   973,  1622,  1385,    65,   458,  4468,    66,\n","        3941,     4,   173,    36,   256,     5,    25,   100,    43,\n","         838,   112,    50,   670, 22665,     9,    35,   480,   284,\n","           5,   150,     4,   172,   112,   167, 21631,   336,   385,\n","          39,     4,   172,  4536,  1111,    17,   546,    38,    13,\n","         447,     4,   192,    50,    16,     6,   147,  2025,    19,\n","          14,    22,     4,  1920,  4613,   469,     4,    22,    71,\n","          87,    12,    16,    43,   530,    38,    76,    15,    13,\n","        1247,     4,    22,    17,   515,    17,    12,    16,   626,\n","          18, 19193,     5,    62,   386,    12,     8,   316,     8,\n","         106,     5,     4,  2223,  5244,    16,   480,    66,  3785,\n","          33,     4,   130,    12,    16,    38,   619,     5,    25,\n","         124,    51,    36,   135,    48,    25,  1415,    33,     6,\n","          22,    12,   215,    28,    77,    52,     5,    14,   407,\n","          16,    82, 10311,     8,     4,   107,   117,  5952,    15,\n","         256,     4, 31050,     7,  3766,     5,   723,    36,    71,\n","          43,   530,   476,    26,   400,   317,    46,     7,     4,\n","       12118,  1029,    13,   104,    88,     4,   381,    15,   297,\n","          98,    32,  2071,    56,    26,   141,     6,   194,  7486,\n","          18,     4,   226,    22,    21,   134,   476,    26,   480,\n","           5,   144,    30,  5535,    18,    51,    36,    28,   224,\n","          92,    25,   104,     4,   226,    65,    16,    38,  1334,\n","          88,    12,    16,   283,     5,    16,  4472,   113,   103,\n","          32,    15,    16,  5345,    19,   178,    32], dtype=int32)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Creating the Model\n","Now it's time to create the model. We'll use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment. \n","\n","32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like!"],"metadata":{"id":"Y-IkdL4CKl4Z"}},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n","])"],"metadata":{"id":"c5Yb2ezLKjqM","executionInfo":{"status":"ok","timestamp":1677707609477,"user_tz":-60,"elapsed":2584,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujay0rmmKnk_","executionInfo":{"status":"ok","timestamp":1677707617687,"user_tz":-60,"elapsed":390,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"6510b858-045b-4d9c-ea34-a2868ccada7c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 32)          2834688   \n","                                                                 \n"," lstm (LSTM)                 (None, 32)                8320      \n","                                                                 \n"," dense (Dense)               (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2,843,041\n","Trainable params: 2,843,041\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### Training\n","Now it's time to compile and train the model. "],"metadata":{"id":"mxm7xQ7vKsIW"}},{"cell_type":"code","source":["model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n","\n","history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBKpigTiKqIl","executionInfo":{"status":"ok","timestamp":1677708378645,"user_tz":-60,"elapsed":746602,"user":{"displayName":"Nawaz Rahman","userId":"05408412285411984916"}},"outputId":"410b90b6-db58-4734-dd87-0e83175dc19b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 77s 116ms/step - loss: 0.4535 - acc: 0.7781 - val_loss: 0.3076 - val_acc: 0.8742\n","Epoch 2/10\n","625/625 [==============================] - 71s 113ms/step - loss: 0.2611 - acc: 0.8992 - val_loss: 0.2816 - val_acc: 0.8836\n","Epoch 3/10\n","625/625 [==============================] - 70s 112ms/step - loss: 0.2004 - acc: 0.9273 - val_loss: 0.3202 - val_acc: 0.8706\n","Epoch 4/10\n","625/625 [==============================] - 69s 111ms/step - loss: 0.1653 - acc: 0.9413 - val_loss: 0.3304 - val_acc: 0.8788\n","Epoch 5/10\n","625/625 [==============================] - 68s 108ms/step - loss: 0.1344 - acc: 0.9531 - val_loss: 0.3010 - val_acc: 0.8892\n","Epoch 6/10\n","625/625 [==============================] - 70s 111ms/step - loss: 0.1124 - acc: 0.9617 - val_loss: 0.3187 - val_acc: 0.8822\n","Epoch 7/10\n","625/625 [==============================] - 72s 116ms/step - loss: 0.0965 - acc: 0.9679 - val_loss: 0.3550 - val_acc: 0.8882\n","Epoch 8/10\n","625/625 [==============================] - 69s 111ms/step - loss: 0.0801 - acc: 0.9747 - val_loss: 0.3884 - val_acc: 0.8824\n","Epoch 9/10\n","625/625 [==============================] - 69s 111ms/step - loss: 0.0700 - acc: 0.9790 - val_loss: 0.4427 - val_acc: 0.8576\n","Epoch 10/10\n","625/625 [==============================] - 69s 111ms/step - loss: 0.0616 - acc: 0.9814 - val_loss: 0.4673 - val_acc: 0.8798\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rbrkS7U5KtlC"},"execution_count":null,"outputs":[]}]}